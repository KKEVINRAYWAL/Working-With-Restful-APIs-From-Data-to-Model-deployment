{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Building Our Data Module\n",
    "\n",
    "\n",
    "| URL | Component |\n",
    "|:--- | :-------- |\n",
    "| `https://www.alphavantage.co` | This is the **hostname** or **base URL**. It is the web address for the server where we can get our stock data. |\n",
    "| `/query` | This is the **path**. Most APIs have lots of different operations they can do. The path is the name of the particular operation we want to access. |\n",
    "| `?` |  This question mark denotes that everything that follows in the URL is a **parameter**. Each parameter is separated by a `&` character. These parameters provide additional information that will change the operation's behavior. This is similar to the way we pass **arguments** into functions in Python. |\n",
    "| `function=TIME_SERIES_DAILY` | Our first parameter uses the `function` keyword. The value is `TIME_SERIES_DAILY`. In this case, we're asking for **daily** stock data. |\n",
    "| `symbol=IBM` | Our second parameter uses the `symbol` keyword. So we're asking for a data on a stock whose [**ticker symbol**](https://en.wikipedia.org/wiki/Ticker_symbol) is `IBM`. |\n",
    "| `apikey=demo` | Much in the same way you need a password to access some websites, an **API key** or **API token** is the password that you'll use to access the API. |\n",
    "\n",
    "## AlphaVantage API Class\n",
    " there is alot that needs to be clarified concerning accessing APIs through a request. \n",
    "\n",
    "```python\n",
    "\n",
    " ticker = \"AMBUJACEM.BSE\"\n",
    "output_size = \"compact\"\n",
    "data_type = \"json\"\n",
    "\n",
    "url = (\n",
    "    \"https://learn-api.wqu.edu/1/data-services/alpha-vantage/query?\"\n",
    "    \"function=TIME_SERIES_DAILY&\"\n",
    "    f\"symbol={ticker}&\"\n",
    "    f\"outputsize={output_size}&\"\n",
    "    f\"datatype={data_type}&\"\n",
    "    f\"apikey={settings.alpha_api_key}\"\n",
    ")\n",
    "\n",
    "print(\"url type:\", type(url))\n",
    "print(url)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data module, we create a class definition for AlphaVantageAPI. For now, making sure that it has an __init__ method that attaches the API key as the attribute __api_key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `AlphaVantageAPI`\n",
    "\n",
    "from data import AlphaVantageAPI\n",
    "# Create instance of `AlphaVantageAPI` class\n",
    "av = AlphaVantageAPI()\n",
    "\n",
    "print(\"av type:\", type(av))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a get_daily method for your AlphaVantageAPI class. Once you're done, use the cell below to fetch the stock data for the renewable energy company Suzlon and assign it to the DataFrame df_suzlon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Suzlon ticker symbol\n",
    "ticker = \"SUZLON.BSE\"\n",
    "\n",
    "# Use your `av` object to get daily data\n",
    "df_suzlon = av.get_daily(ticker=ticker)\n",
    "\n",
    "print(\"df_suzlon type:\", type(df_suzlon))\n",
    "print(\"df_suzlon shape:\", df_suzlon.shape)\n",
    "df_suzlon.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Driven Development \n",
    "validating the  get_daily method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does `get_daily` return a DataFrame?\n",
    "assert isinstance(df_suzlon, pd.DataFrame)\n",
    "\n",
    "# Does DataFrame have 5 columns?\n",
    "assert df_suzlon.shape[1]==5\n",
    "\n",
    "# Does DataFrame have a DatetimeIndex?\n",
    "assert isinstance(df_suzlon.index, pd.DatetimeIndex)\n",
    "\n",
    "# Is the index name \"date\"?\n",
    "assert df_suzlon.index.name== \"date\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further  tests for the output of  get_daily method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does DataFrame have correct column names?\n",
    "\n",
    "assert df_suzlon.columns.to_list()==['open', 'high', 'low', 'close', 'volume']\n",
    "# Are columns correct data type?\n",
    "assert all(df_suzlon.dtypes==float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Repository Class\n",
    "It wouldn't be efficient if our application needed to get data from the AlphaVantage API every time we wanted to explore our data or build a model, so we'll need to store our data in a database. Because our data is highly structured (each DataFrame we extract from AlphaVantage is always going to have the same five columns), it makes sense to use a SQL database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(database=settings.db_name, check_same_thread=False)\n",
    "\n",
    "print(\"connection type:\", type(connection))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for SQLRepository class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class definition\n",
    "from data import SQLRepository\n",
    "# Create instance of class\n",
    "repo = SQLRepository(connection=connection)\n",
    "\n",
    "# Does `repo` have a \"connection\" attribute?\n",
    "assert hasattr(repo, \"connection\")\n",
    "\n",
    "# Is the \"connection\" attribute a SQLite `Connection`?\n",
    "assert isinstance(repo.connection, sqlite3.Connection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a definition for your SQLRepository class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are working on the data module\n",
    "# n_inserted= records.to_sql(\n",
    "#     name=table_name, con=self.connection, if_exists=if_exists\n",
    "# )\n",
    "# return {\n",
    "#     \"transaction_successful\": True,\n",
    "#     \"records_inserted\": n_inserted\n",
    "# }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = repo.insert_table(table_name=ticker, records=df_suzlon, if_exists=\"replace\")\n",
    "\n",
    "# Does your method return a dictionary?\n",
    "assert isinstance(response, dict)\n",
    "\n",
    "# Are the keys of that dictionary correct?\n",
    "assert sorted(list(response.keys())) == [\"records_inserted\", \"transaction_successful\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our method is passing the assert statements, we know it's returning a record of the database transaction, but we still need to check whether the data has actually been added to the database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inserting the data in the database, we can begin to explore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%sql sqlite://///home/path-to-the-SQL-database-fil/stocks.sqlite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%sql: This is a Jupyter magic command that allows you to execute SQL queries directly in a cell. When you begin a line with %sql, the rest of the line will be treated as an SQL query.\n",
    "\n",
    "sqlite:///: This is the connection string indicating the type of database to connect to. In this case, it specifies that the database is of type SQLite. SQLite is a lightweight, self-contained database engine that is often used for local development and testing.\n",
    "\n",
    "/home/path-to-the-SQL-database-file/stocks.sqlite: This is the path to the SQLite database file that the code wants to connect to. In this example, the file is located at the given path. The database file name is stocks.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM 'SUZLON.BSE'\n",
    "LIMIT 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, write a SQL query to get all the Suzlon data. Then use pandas to extract the data from the database and read it into a DataFrame, names df_suzlon_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT * FROM 'SUZLON.BSE'\"\n",
    "df_suzlon_test = pd.read_sql(\n",
    "    sql=sql, con=connection, parse_dates=[\"date\"], index_col=\"date\"\n",
    ")\n",
    "\n",
    "print(\"df_suzlon_test type:\", type(df_suzlon_test))\n",
    "print()\n",
    "print(df_suzlon_test.info())\n",
    "df_suzlon_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to read a table from our database, let's turn our code into a proper function. But since we're doing backwards designs, we need to start with our tests."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDD for the  read_table function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign `read_table` output to `df_suzlon`\n",
    "df_suzlon = read_table(table_name=\"SUZLON.BSE\", limit=2500)  # noQA F821\n",
    "\n",
    "# Is `df_suzlon` a DataFrame?\n",
    "assert isinstance (df_suzlon, pd.DataFrame)\n",
    "\n",
    "# Does it have a `DatetimeIndex`?\n",
    "assert isinstance(df_suzlon.index, pd.DatetimeIndex)\n",
    "\n",
    "# Is the index named \"date\"?\n",
    "assert df_suzlon.index.name==\"date\"\n",
    "\n",
    "# Does it have 2,500 rows and 5 columns?\n",
    "assert df_suzlon.shape==(2500, 5)\n",
    "# Are the column names correct?\n",
    "assert df_suzlon.columns.to_list()==['open', 'high', 'low', 'close', 'volume']\n",
    "# Are the column data types correct?\n",
    "\n",
    "assert all(df_suzlon.dtypes==float)\n",
    "# Print `df_suzlon` info\n",
    "print(\"df_suzlon shape:\", df_suzlon.shape)\n",
    "print()\n",
    "print(df_suzlon.info())\n",
    "df_suzlon.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets write the read_table function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(table_name, limit=None):\n",
    "\n",
    "    \"\"\"Read table from database.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_name : str\n",
    "        Name of table in SQLite database.\n",
    "    limit : int, None, optional\n",
    "        Number of most recent records to retrieve. If `None`, all\n",
    "        records are retrieved. By default, `None`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Index is DatetimeIndex \"date\". Columns are 'open', 'high',\n",
    "        'low', 'close', and 'volume'. All columns are numeric.\n",
    "    \"\"\"\n",
    "    # Create SQL query (with optional limit)\n",
    "    if limit:\n",
    "        sql = f\"SELECT * FROM '{table_name}' LIMIT {limit}\"  # Added space after table_name and before LIMIT keyword\n",
    "    else:\n",
    "        sql = f\"SELECT * FROM '{table_name}'\"\n",
    "\n",
    "    # Retrieve data, read into DataFrame\n",
    "    df = pd.read_sql(\n",
    "        sql=sql, con=connection, parse_dates=[\"date\"], index_col=\"date\"\n",
    "    )\n",
    "\n",
    "    # Return DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDD for read_table Method in the AlphaVantageAPI class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign `read_table` output to `df_suzlon`\n",
    "df_suzlon = repo.read_table(table_name=\"SUZLON.BSE\", limit=2500)  # noQA F821\n",
    "\n",
    "# Is `df_suzlon` a DataFrame?\n",
    "assert isinstance (df_suzlon, pd.DataFrame)\n",
    "\n",
    "# Does it have a `DatetimeIndex`?\n",
    "assert isinstance(df_suzlon.index, pd.DatetimeIndex)\n",
    "\n",
    "# Is the index named \"date\"?\n",
    "assert df_suzlon.index.name==\"date\"\n",
    "\n",
    "# Does it have 2,500 rows and 5 columns?\n",
    "assert df_suzlon.shape==(2500, 5)\n",
    "# Are the column names correct?\n",
    "assert df_suzlon.columns.to_list()==['open', 'high', 'low', 'close', 'volume']\n",
    "# Are the column data types correct?\n",
    "\n",
    "assert all(df_suzlon.dtypes==float)\n",
    "# Print `df_suzlon` info\n",
    "print(\"df_suzlon shape:\", df_suzlon.shape)\n",
    "print()\n",
    "print(df_suzlon.info())\n",
    "df_suzlon.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LETS START USING THE instances of the AlphaVantageAPI and SQLRepository classes(av and repo, respectively) to get the stock data for Ambuja Cement and read it into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"AMBUJACEM.BSE\"\n",
    "\n",
    "# Get Ambuja data using `av`\n",
    "ambuja_records = av.get_daily(ticker=ticker)\n",
    "\n",
    "# Insert `ambuja_records` database using `repo`\n",
    "response = repo.insert_table(\n",
    "    table_name=ticker, records=ambuja_records, if_exists=\"replace\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the read_table method in the  SQLRepository class, extract the most recent 2,500 rows of data for Ambuja Cement from the database and assign the result to df_ambuja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"AMBUJACEM.BSE\"\n",
    "df_ambuja = repo.read_table(table_name=ticker, limit=2500)\n",
    "\n",
    "print(\"df_ambuja type:\", type(df_ambuja))\n",
    "print(\"df_ambuja shape:\", df_ambuja.shape)\n",
    "df_ambuja.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exploring the data to check for validity and if they are fit for use: for this i have ploted the clossing price vs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "# Plot `df_ambuja` closing price\n",
    "df_ambuja[\"close\"].plot(ax=ax, label=\"AMBUJACEM\", color=\"C1\")\n",
    "\n",
    "# Label axes\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"Closing Price\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a plot that shows the closing prices of df_suzlon and df_ambuja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "# Plot `df_suzlon` and `df_ambuja`\n",
    "df_suzlon[\"close\"].plot(ax=ax, label=\"SUZLON\")\n",
    "df_ambuja[\"close\"].plot(ax=ax, label=\"AMBUJACEM\")\n",
    "# Label axes\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Closing Price\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One way in which investors compare stocks is by looking at their returns instead. A return is the change in value in an investment, represented as a percentage. So let's look at the daily returns for our two stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame ascending by date\n",
    "df_ambuja.sort_index(ascending=True, inplace=True)\n",
    "# Create \"return\" column\n",
    "\n",
    "df_ambuja[\"return\"]=df_ambuja[\"close\"].pct_change()*100\n",
    "print(\"df_ambuja shape:\", df_ambuja.shape)\n",
    "print(df_ambuja.info())\n",
    "df_ambuja.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets do for suzlon as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame ascending by date\n",
    "df_suzlon.sort_index(ascending=True, inplace=True)\n",
    "# Create \"return\" column\n",
    "\n",
    "df_suzlon[\"return\"]=df_suzlon[\"close\"].pct_change()*100\n",
    "print(\"df_ambuja shape:\", df_suzlon.shape)\n",
    "print(df_suzlon.info())\n",
    "df_suzlon.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the returns for df_suzlon and df_ambuja. Be sure to label your axes and use legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "# Plot `df_suzlon` and `df_ambuja`\n",
    "df_suzlon[\"return\"].plot(ax=ax, label=\"SUZLON\")\n",
    "df_ambuja[\"return\"].plot(ax=ax, label=\"AMBUJACEM\")\n",
    "# Label axes\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Return\")\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we can plot the two data comparatively in one plot, we achieve great insights about them, especially how they compare in terms of spread. This is what investors look at are often refer to as volatility."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING THE garch model for Predicting Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from arch import arch_model\n",
    "from config import settings\n",
    "from data import SQLRepository\n",
    "from IPython.display import VimeoVideo\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's connect to the database  and then instantiate the SQLRepository named repo to interact with that database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(settings.db_name, check_same_thread=False)\n",
    "repo = SQLRepository(connection=connection)\n",
    "\n",
    "print(\"repo type:\", type(repo))\n",
    "print(\"repo.connection type:\", type(repo.connection))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Pull the most recent 2,500 rows of data for Ambuja Cement from the database. Assign the results to the variable df_ambuja.\n",
    "\n",
    "we can also Inspect a DataFrame using shape, info, and head in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ambuja = repo.read_table(table_name=\"AMBUJACEM.BSE\", limit=2500)\n",
    "\n",
    "print(\"df_ambuja type:\", type(df_ambuja))\n",
    "print(\"df_ambuja shape:\", df_ambuja.shape)\n",
    "df_ambuja.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a wrangle_data function whose output is the returns for a stock stored in your database. Use the docstring as a guide and the assert statements in the following code block to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_data(ticker, n_observations):\n",
    "\n",
    "    \"\"\"Extract table data from database. Calculate returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ticker : str\n",
    "        The ticker symbol of the stock (also table name in database).\n",
    "\n",
    "    n_observations : int\n",
    "        Number of observations to return.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Name will be `\"return\"`. There will be no `NaN` values.\n",
    "    \"\"\"\n",
    "    # Get table from database\n",
    "    df=repo.read_table(table_name=ticker, limit=n_observations+1)\n",
    "\n",
    "\n",
    "    # Sort DataFrame ascending by date\n",
    "    df.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "    # Create \"return\" column\n",
    "    df[\"return\"]=df[\"close\"].pct_change() *100\n",
    "\n",
    "\n",
    "    # Return returns\n",
    "    return df[\"return\"].dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the cell below to test your function, you'll also create a Series y_ambuja that we'll use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ambuja = wrangle_data(ticker=\"AMBUJACEM.BSE\", n_observations=2500)\n",
    "\n",
    "# Is `y_ambuja` a Series?\n",
    "assert isinstance(y_ambuja, pd.Series)\n",
    "\n",
    "# Are there 2500 observations in the Series?\n",
    "assert len(y_ambuja) == 2500\n",
    "\n",
    "# Is `y_ambuja` name \"return\"?\n",
    "assert y_ambuja.name == \"return\"\n",
    "\n",
    "# Does `y_ambuja` have a DatetimeIndex?\n",
    "assert isinstance(y_ambuja.index, pd.DatetimeIndex)\n",
    "\n",
    "# Is index sorted ascending?\n",
    "assert all(y_ambuja.index == y_ambuja.sort_index(ascending=True).index)\n",
    "\n",
    "# Are there no `NaN` values?\n",
    "assert y_ambuja.isnull().sum() == 0\n",
    "\n",
    "y_ambuja.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore\n",
    "Let's recreate the volatility time series plot we made in the last lesson so that we have a visual aid to talk about what volatility is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot returns for `df_suzlon` and `df_ambuja`\n",
    "y_suzlon.plot(ax=ax, label=\"SUZLON\")\n",
    "y_ambuja.plot(ax=ax, label=\"AMBUJACEM\")\n",
    "\n",
    "# Label axes\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Return\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows how returns change over time. This may seem like a totally new concept, but if we visualize them without considering time, things will start to look familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of `y_ambuja`, 25 bins\n",
    "plt.hist(y_ambuja, bins=25)\n",
    "# Add axis labels\n",
    "\n",
    "plt.xlabel(\"Daily Returns\")\n",
    "plt.ylabel(\"Frequency [Count]\")\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Distribution of Ambuja Cement Daily Returns\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volatility is the measure of the spread of these returns around the mean. In other words, volatility in finance is the same thing at standard deviation in statistics.\n",
    "\n",
    "Let's start by measuring the daily volatility of our two stocks. Since our data frequency is also daily, this will be exactly the same as calculating the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suzlon_daily_volatility = y_suzlon.std()\n",
    "ambuja_daily_volatility = y_ambuja.std()\n",
    "\n",
    "print(\"Suzlon Daily Volatility:\", suzlon_daily_volatility)\n",
    "print(\"Ambuja Daily Volatility:\", ambuja_daily_volatility)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Suzlon is more volatile than Ambuja. This reinforces what we saw in our time series plot, where Suzlon returns have a much wider spread.\n",
    "\n",
    "While daily volatility is useful, investors are also interested in volatility over other time periods — like annual volatility. Keep in mind that a year isn't 365 days for a stock market, though. After excluding weekends and holidays, most markets have only 252 trading days."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the annual volatility for Suzlon and Ambuja, assigning the results to suzlon_annual_volatility and ambuja_annual_volatility, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suzlon_annual_volatility = suzlon_daily_volatility* np.sqrt(252)\n",
    "ambuja_annual_volatility = ambuja_daily_volatility * np.sqrt(252)\n",
    "\n",
    "print(\"Suzlon Annual Volatility:\", suzlon_annual_volatility)\n",
    "print(\"Ambuja Annual Volatility:\", ambuja_annual_volatility)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the rolling volatility for y_ambuja, using a 50-day window. Assign the result to ambuja_rolling_50d_volatility.\n",
    "\n",
    "What's a rolling window?\n",
    "Do a rolling window calculation in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot `y_ambuja`\n",
    "\n",
    "y_ambuja.plot(ax=ax, label=\"daily return\")\n",
    "# Plot `ambuja_rolling_50d_volatility`\n",
    "ambuja_rolling_50d_volatility.plot(ax=ax, label=\"50d rolling volatility\", linewidth=3 )\n",
    "\n",
    "# Add x-axis label\n",
    "plt.xlabel(\"Date\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that volatility goes up when the returns change drastically — either up or down. For instance, we can see a big increase in volatility in May 2020, when there were several days of large negative returns. We can also see volatility go down in August 2022, when there are only small day-to-day changes in returns.\n",
    "\n",
    "This plot reveals a problem. We want to use returns to see if high volatility on one day is associated with high volatility on the following day. But high volatility is caused by large changes in returns, which can be either positive or negative. How can we assess negative and positive numbers together without them canceling each other out? One solution is to take the absolute value of the numbers, which is what we do to calculate performance metrics like mean absolute error. The other solution, which is more common in this context, is to square all the values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a time series plot of the squared returns in y_ambuja. Don't forget to label your axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot squared returns\n",
    "\n",
    "(y_ambuja**2).plot(ax=ax)\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Squared Returns\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now it's much easier to see that (1) we have periods of high and low volatility, and (2) high volatility days tend to cluster together. This is a perfect situation to use a GARCH model.\n",
    "\n",
    "A GARCH model is sort of like the ARMA model we learned about in Lesson 3.4. It has a p parameter handling correlations at prior time steps and a q parameter for dealing with \"shock\" events. It also uses the notion of lag. To see how many lags we should have in our model, we should create an ACF and PACF plot — but using the squared returns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ACF plot of squared returns for Ambuja Cement. Be sure to label your x-axis \"Lag [days]\" and your y-axis \"Correlation Coefficient\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create ACF of squared returns\n",
    "plot_acf(y_ambuja**2, ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Lag [Days]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PACF plot of squared returns for Ambuja Cement. Be sure to label your x-axis \"Lag [days]\" and your y-axis \"Correlation Coefficient\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create PACF of squared returns\n",
    "plot_pacf(y_ambuja**2, ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Lag [Days]\")\n",
    "plt.ylabel(\"Correlation Coefficient\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split\n",
    "The last thing we need to do before building our model is to create a training set. Note that we won't create a test set here. Rather, we'll use all of y_ambuja to conduct walk-forward validation after we've built our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training set y_ambuja_train that contains the first 80% of the observations in y_ambuja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_test = int(len(y_ambuja)*0.8)\n",
    "y_ambuja_train = y_ambuja.iloc[:cutoff_test]\n",
    "\n",
    "print(\"y_ambuja_train type:\", type(y_ambuja_train))\n",
    "print(\"y_ambuja_train shape:\", y_ambuja_train.shape)\n",
    "y_ambuja_train.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train model\n",
    "model = arch_model(\n",
    "    y_ambuja_train, \n",
    "    p=2,\n",
    "    q=2, \n",
    "    rescale=False\n",
    "    \n",
    ").fit(disp=0)\n",
    "print(\"model type:\", type(model))\n",
    "\n",
    "# Show model summary\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a time series plot with the Ambuja returns and the conditional volatility for your model. Be sure to include axis labels and add a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot `y_ambuja_train`\n",
    "\n",
    "y_ambuja_train.plot(ax=ax, label=\"Ambuja Daily Returns\")\n",
    "# Plot conditional volatility * 2\n",
    "(2* model.conditional_volatility).plot(\n",
    "    ax=ax, color=\"C1\", label=\"2 SD Conditional Volatility\", linewidth=3\n",
    ")\n",
    "\n",
    "# Plot conditional volatility * -2\n",
    "\n",
    "(-2* model.conditional_volatility).plot(\n",
    "    ax=ax, color=\"C1\", label=\"2 SD Conditional Volatility\", linewidth=3\n",
    ")\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Date\")\n",
    "\n",
    "\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a time series plot of the standardized residuals for your model. Be sure to include axis labels and a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot standardized residuals\n",
    "model.std_resid.plot(ax=ax, label=\"Standardized Residuals\")\n",
    "\n",
    "# Add axis labels\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a histogram with 25 bins of the standardized residuals for your model. Be sure to label your axes and use a title.\n",
    "\n",
    "What's a histogram?\n",
    "Create a histogram using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of standardized residuals, 25 bins\n",
    "plt.hist(model.std_resid, bins=25)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Standardized Residuals\")\n",
    "\n",
    "plt.ylabel(\"Frequency[count]\")\n",
    "# Add title\n",
    "plt.title(\"Distribution of standard residuals\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ACF plot of the square of your standardized residuals. Don't forget axis labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create ACF of squared, standardized residuals\n",
    "plot_acf(model.std_resid**2, ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "\n",
    "plt.xlabel(\"Lag [days]\")\n",
    "plt.ylabel(\"Correlation Coefficient\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a one-day forecast from your model and assign the result to the variable one_day_forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_forecast = model.forecast(horizon=1, reindex=False).variance\n",
    "\n",
    "print(\"one_day_forecast type:\", type(one_day_forecast))\n",
    "one_day_forecast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to hold predictions\n",
    "predictions = []\n",
    "\n",
    "# Calculate size of test data (20%)\n",
    "test_size = int(len(y_ambuja) * 0.2)\n",
    "\n",
    "# Walk forward\n",
    "for i in range(test_size):\n",
    "    # Create test data\n",
    "    y_train = y_ambuja.iloc[: -(test_size - i)]\n",
    "\n",
    "    # Train model\n",
    "    model = arch_model(y_train, p=1, q=1, rescale=False).fit(disp=0)\n",
    "\n",
    "    # Generate next prediction (volatility, not variance)\n",
    "    next_pred = model.forecast(horizon=1, reindex=False).variance.iloc[0,0]**0.5\n",
    "\n",
    "    # Append prediction to list\n",
    "    predictions.append(next_pred)\n",
    "\n",
    "# Create Series from predictions list\n",
    "y_test_wfv = pd.Series(predictions, index=y_ambuja.tail(test_size).index)\n",
    "\n",
    "print(\"y_test_wfv type:\", type(y_test_wfv))\n",
    "print(\"y_test_wfv shape:\", y_test_wfv.shape)\n",
    "y_test_wfv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Plot returns for test data\n",
    "y_ambuja.tail(test_size).plot(ax=ax, label=\"Ambuja Return\")\n",
    "\n",
    "# Plot volatility predictions * 2\n",
    "(2 * y_test_wfv).plot(ax=ax, c=\"C1\", label=\"2 SD Predicted Volatility\")\n",
    "\n",
    "# Plot volatility predictions * -2\n",
    "(-2 * y_test_wfv).plot(ax=ax, c=\"C1\")\n",
    "\n",
    "# Label axes\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Return\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5-day volatility forecast\n",
    "prediction = model.forecast(horizon=5, reindex=False).variance ** 0.5\n",
    "print(prediction)\n",
    "\n",
    "# Calculate forecast start date\n",
    "start = prediction.index[0]+pd.DateOffset(days=1)\n",
    "\n",
    "# Create date range\n",
    "prediction_dates = pd.bdate_range(start=start, periods=prediction.shape[1])\n",
    "\n",
    "# Create prediction index labels, ISO 8601 format\n",
    "prediction_index = [d.isoformat() for d in prediction_dates]\n",
    "\n",
    "print(\"prediction_index type:\", type(prediction_index))\n",
    "print(\"prediction_index len:\", len(prediction_index))\n",
    "prediction_index[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prediction(prediction):\n",
    "\n",
    "    \"\"\"Reformat model prediction to JSON.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : pd.DataFrame\n",
    "        Variance from a `ARCHModelForecast`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Forecast of volatility. Each key is date in ISO 8601 format.\n",
    "        Each value is predicted volatility.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Calculate forecast start date\n",
    "    start = prediction.index[0]+pd.DateOffset(days=1)\n",
    "\n",
    "    # Create date range\n",
    "    prediction_dates = pd.bdate_range(start=start, periods=prediction.shape[1])\n",
    "\n",
    "    # Create prediction index labels, ISO 8601 format\n",
    "    prediction_index = [d.isoformat() for d in prediction_dates]\n",
    "\n",
    "    # Extract predictions from DataFrame, get square root\n",
    "    data=prediction.values.flatten()**0.5\n",
    "    # Combine `data` and `prediction_index` into Series\n",
    "    prediction_formatted= pd.Series(data, index=prediction_index)\n",
    "\n",
    "\n",
    "    # Return Series as dictionary\n",
    "    return prediction_formatted.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.forecast(horizon=10, reindex=False).variance\n",
    "prediction_formatted = clean_prediction(prediction)\n",
    "\n",
    "# Is `prediction_formatted` a dictionary?\n",
    "assert isinstance(prediction_formatted, dict)\n",
    "\n",
    "# Are keys correct data type?\n",
    "assert all(isinstance(k, str) for k in prediction_formatted.keys())\n",
    "\n",
    "# Are values correct data type\n",
    "assert all(isinstance(v, float) for v in prediction_formatted.values())\n",
    "\n",
    "prediction_formatted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "at this stage We have a module for getting and storing our data. We have the code to train model and clean its predictions. Lets put them all   together and deploy the model with an API that others can use to train their own models and predict volatility. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a SQLRepository named repo.\n",
    "\n",
    "Open a connection to a SQL database using sqlite3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(settings.db_name, check_same_thread=False)\n",
    "repo = SQLRepository(connection=connection)\n",
    "\n",
    "print(\"repo type:\", type(repo))\n",
    "print(\"repo.connection type:\", type(repo.connection))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `model` module, create a definition for a `GarchModel` model class. For now, it should only have an `__init__` method. Use the docstring as a guide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GarchModel\n",
    "\n",
    "# Instantiate a `GarchModel`\n",
    "gm_ambuja = GarchModel(ticker=\"AMBUJACEM.BSE\", repo=repo, use_new_data=False)\n",
    "\n",
    "# Does `gm_ambuja` have the correct attributes?\n",
    "assert gm_ambuja.ticker == \"AMBUJACEM.BSE\"\n",
    "assert gm_ambuja.repo == repo\n",
    "assert not gm_ambuja.use_new_data\n",
    "assert gm_ambuja.model_directory == settings.model_directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate `GarchModel`, use new data\n",
    "model_shop = GarchModel(ticker=\"SHOPERSTOP.BSE\", repo=repo, use_new_data=True)\n",
    "\n",
    "# Check that model doesn't have `data` attribute yet\n",
    "assert not hasattr(model_shop, \"data\")\n",
    "\n",
    "# Wrangle data\n",
    "model_shop.wrangle_data(n_observations=1000)\n",
    "\n",
    "# Does model now have `data` attribute?\n",
    "assert hasattr(model_shop, \"data\")\n",
    "\n",
    "# Is the `data` a Series?\n",
    "assert isinstance(model_shop.data, pd.Series)\n",
    "\n",
    "# Is Series correct shape?\n",
    "assert model_shop.data.shape == (1000,)\n",
    "\n",
    "model_shop.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wrangle_data(self, n_observations):\n",
    "\n",
    "#         \"\"\"Extract data from database (or get from AlphaVantage), transform it\n",
    "#         for training model, and attach it to `self.data`.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         n_observations : int\n",
    "#             Number of observations to retrieve from database\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         None\n",
    "#         \"\"\"\n",
    "#         # Add new data to database if required\n",
    "#         if self.use_new_data:\n",
    "#             #instantiate an API class\n",
    "#             api=AlphaVantageAPI()\n",
    "#             #Get Data\n",
    "#             new_data=api.get_daily(ticker=self.ticker)\n",
    "#             #insert data into repo\n",
    "#             self.repo.insert_table(\n",
    "#                 table_name=self.ticker, records=new_data, if_exists=\"replace\"\n",
    "#             )\n",
    "\n",
    "#         # Pull data from SQL database\n",
    "#         df=self.repo.read_table(table_name=self.ticker, limit=n_observations+1)\n",
    "#         # Clean data, attach to class as `data` attribute\n",
    "#         df.sort_index(ascending=True, inplace=True)\n",
    "#         df[\"return\"]=df[\"close\"].pct_change() *100\n",
    "#         self.data = df[\"return\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate `GarchModel`, use new data\n",
    "model_shop = GarchModel(ticker=\"SHOPERSTOP.BSE\", repo=repo, use_new_data=True)\n",
    "\n",
    "# Check that model doesn't have `data` attribute yet\n",
    "assert not hasattr(model_shop, \"data\")\n",
    "\n",
    "# Wrangle data\n",
    "model_shop.wrangle_data(n_observations=1000)\n",
    "\n",
    "# Does model now have `data` attribute?\n",
    "assert hasattr(model_shop, \"data\")\n",
    "\n",
    "# Is the `data` a Series?\n",
    "assert isinstance(model_shop.data, pd.Series)\n",
    "\n",
    "# Is Series correct shape?\n",
    "assert model_shop.data.shape == (1000,)\n",
    "\n",
    "model_shop.data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using your code from the previous lesson, create a `fit` method for your `GarchModel` class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit(self,p, q):\n",
    "\n",
    "#         \"\"\"Create model, fit to `self.data`, and attach to `self.model` attribute.\n",
    "#         For assignment, also assigns adds metrics to `self.aic` and `self.bic`.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         p : int\n",
    "#             Lag order of the symmetric innovation\n",
    "\n",
    "#         q : ind\n",
    "#             Lag order of lagged volatility\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         None\n",
    "#         \"\"\"\n",
    "#         # Train Model, attach to `self.model`\n",
    "#         self.model = arch_model(self.data, p=p, q=q, rescale=False).fit(disp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate `GarchModel`, use old data\n",
    "model_shop = GarchModel(ticker=\"SHOPERSTOP.BSE\", repo=repo, use_new_data=False)\n",
    "\n",
    "# Wrangle data\n",
    "model_shop.wrangle_data(n_observations=1000)\n",
    "\n",
    "# Fit GARCH(1,1) model to data\n",
    "model_shop.fit(p=1, q=1)\n",
    "\n",
    "# Does `model_shop` have a `model` attribute now?\n",
    "assert hasattr(model_shop, \"model\")\n",
    "\n",
    "# Is model correct data type?\n",
    "assert isinstance(model_shop.model, ARCHModelResult)\n",
    "\n",
    "# Does model have correct parameters?\n",
    "assert model_shop.model.params.index.tolist() == [\"mu\", \"omega\", \"alpha[1]\", \"beta[1]\"]\n",
    "\n",
    "# Check model parameters\n",
    "model_shop.model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a `predict_volatility` method for your `GarchModel` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __clean_prediction(self, prediction):\n",
    "\n",
    "#         \"\"\"Reformat model prediction to JSON.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         prediction : pd.DataFrame\n",
    "#             Variance from a `ARCHModelForecast`\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         dict\n",
    "#             Forecast of volatility. Each key is date in ISO 8601 format.\n",
    "#             Each value is predicted volatility.\n",
    "#         \"\"\"\n",
    "#         # Calculate forecast start date\n",
    "#         start = prediction.index[0]+pd.DateOffset(days=1)\n",
    "\n",
    "#         # Create date range\n",
    "#         prediction_dates = pd.bdate_range(start=start, periods=prediction.shape[1])\n",
    "\n",
    "#         # Create prediction index labels, ISO 8601 format\n",
    "#         prediction_index = [d.isoformat() for d in prediction_dates]\n",
    "\n",
    "#         # Extract predictions from DataFrame, get square root\n",
    "#         data=prediction.values.flatten()**0.5\n",
    "#         # Combine `data` and `prediction_index` into Series\n",
    "#         prediction_formatted= pd.Series(data, index=prediction_index)\n",
    "\n",
    "#         # Return Series as dictionary\n",
    "#         return prediction_formatted.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_volatility(self, horizon):\n",
    "\n",
    "#         \"\"\"Predict volatility using `self.model`\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         horizon : int\n",
    "#             Horizon of forecast, by default 5.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         dict\n",
    "#             Forecast of volatility. Each key is date in ISO 8601 format.\n",
    "#             Each value is predicted volatility.\n",
    "#         \"\"\"\n",
    "#         # Generate variance forecast from `self.model`\n",
    "#         prediction = self.model.forecast(horizon=horizon, reindex=False).variance\n",
    "\n",
    "#         # Format prediction with `self.__clean_predction`\n",
    "#         prediction_formatted = self.__clean_prediction(prediction)\n",
    "\n",
    "#         # Return `prediction_formatted`\n",
    "#         return prediction_formatted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a `dump` method for your `GarchModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def dump(self):\n",
    "\n",
    "#         \"\"\"Save model to `self.model_directory` with timestamp.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         str\n",
    "#             filepath where model was saved.\n",
    "#         \"\"\"\n",
    "#         # Create timestamp in ISO format\n",
    "#         timestamp = pd.Timestamp.now().isoformat()\n",
    "#         # Create filepath, including `self.model_directory`\n",
    "#         filepath = os.path.join(self.model_directory, f\"{timestamp}_{self.ticker}.pkl\")\n",
    "#         # Save `self.model`\n",
    "#         joblib.dump(self.model, filepath)\n",
    "\n",
    "#         # Return filepath\n",
    "#         return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save `model_shop` model, assign filename\n",
    "filename = model_shop.dump()\n",
    "\n",
    "# Is `filename` a string?\n",
    "assert isinstance(filename, str)\n",
    "\n",
    "# Does filename include ticker symbol?\n",
    "assert model_shop.ticker in filename\n",
    "\n",
    "# Does file exist?\n",
    "assert os.path.exists(filename)\n",
    "\n",
    "filename"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a `load` function below that will take a ticker symbol as input and return a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(ticker):\n",
    "\n",
    "    \"\"\"Load latest model from model directory.\n",
    "\n",
    "    Parameters\n",
    "     ----------\n",
    "    ticker : str\n",
    "    Ticker symbol for which model was trained.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `ARCHModelResult`\n",
    "    \"\"\"\n",
    "    # Create pattern for glob search\n",
    "    pattern= os.path.join(settings.model_directory, f\"*{ticker}.pkl\")\n",
    "\n",
    "     # Try to find path of latest model\n",
    "    try:\n",
    "        model_path=sorted(glob(pattern))[-1]\n",
    "    # Handle possible `IndexError`\n",
    "    except IndexError:\n",
    "        raise Exception(f\"No Model trained for '{ticker}'.\" )\n",
    "    # Load model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # Return model\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN MODULE \n",
    "Similar to the interactive applications we made in Projects 6 and 7, our first step here will be to create an `app` object. This time, instead of being a plotly application, it'll be a FastAPI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the `main` module, instantiate a FastAPI application named `app`.\n",
    "\n",
    "- [Instantiate an application in FastAPI.](../%40textbook/22-apis.ipynb#Creating-a-Path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cmd on  the directory for the s project, start the app server by entering the following command.\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload --workers 1 --host localhost --port 8008\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:8008/hello\"\n",
    "response = requests.get(url=url)\n",
    "\n",
    "print(\"response code:\", response.status_code)\n",
    "response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `\"/fit\"` Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first path will allow the user to fit a model to stock data when they make a `post` request to our server. They'll have the choice to use new data from AlphaVantage, or older data that's already in our database. When a user makes a request, they'll receive a response telling them if the operation was successful or whether there was an error. \n",
    "\n",
    "One thing that's very important when building an API is making sure the user passes the correct parameters into the app. Otherwise, our app could crash! FastAPI works well with the [pydantic library](https://pydantic-docs.helpmanual.io/), which checks that each request has the correct parameters and data types. It does this by using special data classes that we need to define. Our `\"/fit\"` path will take user input and then output a response, so we need two classes: one for input and one for output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data classes defined, let's see how pydantic ensures our that users are supplying the correct input and our application is returning the correct output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `build_model` function in your `main` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4.16:** Create a `\"/fit\"` path for your `app`. It will take a `FitIn` object as input, and then build a `GarchModel` using the `build_model` function. The model will wrangle the needed data, fit to the data, and save the completed model. Finally, it will send a response in the form of a `FitOut` object. Be sure to handle any errors that may arise. \n",
    "\n",
    "- [Create an application path in FastAPI.](../%40textbook/22-apis.ipynb#Creating-a-Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8.4.16, `\"/fit\" path, 200 status code\n",
    "@app.post(\"/fit\", status_code=200, response_model=FitOut)\n",
    "def fit_model(request: FitIn):\n",
    "    \"\"\"Fit model, return confirmation message.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    request : FitIn\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    dict\n",
    "        Must conform to `FitOut` class\n",
    "    \"\"\"\n",
    "    # Create `response` dictionary from `request`\n",
    "    response=request.dict()\n",
    "    # Create try block to handle exceptions\n",
    "    try:\n",
    "        # Build model with `build_model` function\n",
    "        model=build_model(ticker=request.ticker, use_new_data=request.use_new_data)\n",
    "        # Wrangle data\n",
    "        model.wrangle_data(n_observations=request.n_observations)\n",
    "        # Fit model\n",
    "        model.fit(p=request.p, q=request.q)\n",
    "        # Save model\n",
    "        filename=model.dump()\n",
    "        # Add `\"success\"` key to `response`\n",
    "        response[\"success\"]=True\n",
    "        # Add `\"message\"` key to `response` with `filename`\n",
    "        response[\"message\"]=f\"Trained and saved '{filename}'.\"\n",
    "    # Create except block\n",
    "    try Exception as e:\n",
    "        # Add `\"success\"` key to `response`\n",
    "        response[\"success\"]= False\n",
    "        # Add `\"message\"` key to `response` with error message\n",
    "        response[\"message\"]=str(e)\n",
    "\n",
    "    # Return response\n",
    "    return response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `\"/predict\"` Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our `\"/predict\"` path, users will be able to make a `post` request with the ticker symbol they want a prediction for and the number of days they want to forecast into the future. Our app will return a forecast or, if there's an error, a message explaining the problem.\n",
    "\n",
    "The setup will be very similar to our `\"/fit\"` path. We'll start with data classes for the in- and output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create definitions for a `PredictIn` and `PredictOut` data class. The `PredictIn` class should inherit from the pydantic `BaseModel`, and the `PredictOut` class should inherit from the `PredictIn` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Task 8.4.18, `PredictIn` class\n",
    "class PredictIn(BaseModel):\n",
    "    ticker:str\n",
    "    n_days:int\n",
    "\n",
    "\n",
    "# Task 8.4.18, `PredictOut` class\n",
    "class PreductOut(PredictIn):\n",
    "    success:bool\n",
    "    forecast:dict\n",
    "    message:str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a `\"/predict\"` path for your `app`. It will take a `PredictIn` object as input, build a `GarchModel`, load the most recent trained model for the given ticker, and generate a dictionary of predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8.4.19 `\"/predict\" path, 200 status code\n",
    "@app.post(\"/predict\", status_code=200, response_mode=PredictOut)\n",
    "def get_prediction(request: PredictIn):\n",
    "\n",
    "    # Create `response` dictionary from `request`\n",
    "    response=request.dict()\n",
    "\n",
    "    # Create try block to handle exceptions\n",
    "    try:\n",
    "        # Build model with `build_model` function\n",
    "        model=build_model(ticker=request.ticker)\n",
    "\n",
    "        # Load stored model\n",
    "        model.load()\n",
    "\n",
    "        # Generate prediction\n",
    "\n",
    "        prediction=model.predict_volatility(horizon=request.n_days)\n",
    "        # Add `\"success\"` key to `response`\n",
    "        response[\"success\"]=True\n",
    "\n",
    "        # Add `\"forecast\"` key to `response`\n",
    "        response[\"forecast\"]=prediction\n",
    "\n",
    "        # Add `\"message\"` key to `response`\n",
    "        response[\"message\"]=str(e)\n",
    "\n",
    "    # Create except block\n",
    "    except Exception as e:\n",
    "        # Add `\"success\"` key to `response`\n",
    "        response[\"success\"]=False\n",
    "\n",
    "        # Add `\"forecast\"` key to `response`\n",
    "        response[\"forecast\"]={}\n",
    "\n",
    "        #  Add `\"message\"` key to `response`\n",
    "        response[\"message\"]=str(e)\n",
    "\n",
    "    # Return response\n",
    "    return response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a `post` request to hit the `\"/predict\"` path running at `\"http://localhost:8008\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of `/predict` path\n",
    "url = \"http://localhost:8008/predict\"\n",
    "# Data to send to path\n",
    "json = {\"ticker\":\"SHOPERSTOP.BSE\", \"n_days\":3}\n",
    "# Response of post request\n",
    "response = requests.post(url=url, json=json)\n",
    "# Response JSON to be submitted to grader\n",
    "submission = response.json()\n",
    "# Inspect JSON\n",
    "submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
